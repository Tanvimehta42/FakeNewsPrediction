{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanvimehta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing packages and libraries \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn import metrics\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'train.csv', sep='\\t', encoding='utf-8')\n",
    "test = pd.read_csv(r'test.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Get the latest from TODAY Sign up for our news...     1\n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...     1\n",
       "2  It’s safe to say that Instagram Stories has fa...     0\n",
       "3  Much like a certain Amazon goddess with a lass...     0\n",
       "4  At a time when the perfect outfit is just one ...     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "label    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0', 'label'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2972\n",
       "1        2014\n",
       "label       1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[train['label']=='label'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = pd.to_numeric(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "label     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = train[train['label']==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[ind,'type']='reliable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = train[train['label']==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[ind,'type']='fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label      type\n",
       "0  Get the latest from TODAY Sign up for our news...      1      fake\n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...      1      fake\n",
       "2  It’s safe to say that Instagram Stories has fa...      0  reliable\n",
       "3  Much like a certain Amazon goddess with a lass...      0  reliable\n",
       "4  At a time when the perfect outfit is just one ...      0  reliable"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.type.value_counts().plot(kind='bar', color='springgreen', alpha=0.5)\n",
    "plt.xlabel('News Type')\n",
    "plt.ylabel('Total Count')\n",
    "plt.title(\"Counts for News Type\", fontsize=25)\n",
    "plt.savefig('newstypecounts.png', bbox_inches='tight', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[\"text\"], y, test_size = 0.33, random_state = 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000000', '000th', '001', '001100', '00113234', '002', '007', '008']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735722964763062\n",
      "[[756 227]\n",
      " [208 455]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78       983\n",
      "           1       0.67      0.69      0.68       663\n",
      "\n",
      "    accuracy                           0.74      1646\n",
      "   macro avg       0.73      0.73      0.73      1646\n",
      "weighted avg       0.74      0.74      0.74      1646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "y_pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "\n",
    "#confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7223572296476306\n",
      "[[941  42]\n",
      " [415 248]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.96      0.80       983\n",
      "           1       0.86      0.37      0.52       663\n",
      "\n",
      "    accuracy                           0.72      1646\n",
      "   macro avg       0.77      0.67      0.66      1646\n",
      "weighted avg       0.76      0.72      0.69      1646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = svm.SVC()\n",
    "svm_classifier.fit(count_train, y_train)\n",
    "y_pred = svm_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "\n",
    "#confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000000', '000th', '001', '001100', '00113234', '002', '007', '008']\n",
      "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.0339453 0.        ... 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7095990279465371\n",
      "[[957  26]\n",
      " [452 211]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.97      0.80       983\n",
      "           1       0.89      0.32      0.47       663\n",
      "\n",
      "    accuracy                           0.71      1646\n",
      "   macro avg       0.78      0.65      0.63      1646\n",
      "weighted avg       0.76      0.71      0.67      1646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "\n",
    "#confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.7168894289185905\n",
      "Alpha:  0.1\n",
      "Score:  0.7679222357229648\n",
      "Alpha:  0.2\n",
      "Score:  0.7618469015795869\n",
      "Alpha:  0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.7624544349939246\n",
      "Alpha:  0.4\n",
      "Score:  0.7448359659781288\n",
      "Alpha:  0.5\n",
      "Score:  0.7345078979343864\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.727217496962333\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.7223572296476306\n",
      "Alpha:  0.8\n",
      "Score:  0.7187120291616039\n",
      "Alpha:  0.9\n",
      "Score:  0.7138517618469016\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    y_pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7679222357229648\n",
      "[[869 114]\n",
      " [268 395]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.82       983\n",
      "           1       0.78      0.60      0.67       663\n",
      "\n",
      "    accuracy                           0.77      1646\n",
      "   macro avg       0.77      0.74      0.75      1646\n",
      "weighted avg       0.77      0.77      0.76      1646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "\n",
    "#confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7624544349939246\n",
      "[[900  83]\n",
      " [308 355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.82       983\n",
      "           1       0.81      0.54      0.64       663\n",
      "\n",
      "    accuracy                           0.76      1646\n",
      "   macro avg       0.78      0.73      0.73      1646\n",
      "weighted avg       0.77      0.76      0.75      1646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = svm.SVC()\n",
    "svm_classifier.fit(tfidf_train, y_train)\n",
    "y_pred = svm_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "\n",
    "#confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(row):\n",
    "    tokens = row.split()\n",
    "    newtokens = [w for w in tokens if w.isalpha()]\n",
    "    newtokens = [w for w in newtokens if w not in punctuation]\n",
    "    return newtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['no_punc_content'] = train.text.apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>no_punc_content</th>\n",
       "      <th>stemmed_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>[Get, the, latest, from, TODAY, Sign, up, for,...</td>\n",
       "      <td>[get, the, latest, from, today, sign, up, for,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>[Conan, On, The, Funeral, Trump, Will, Be, Inv...</td>\n",
       "      <td>[conan, On, the, funer, trump, will, Be, invit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[safe, to, say, that, Instagram, Stories, has,...</td>\n",
       "      <td>[safe, to, say, that, instagram, stori, ha, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[Much, like, a, certain, Amazon, goddess, with...</td>\n",
       "      <td>[much, like, a, certain, amazon, goddess, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[At, a, time, when, the, perfect, outfit, is, ...</td>\n",
       "      <td>[At, a, time, when, the, perfect, outfit, is, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label      type  \\\n",
       "0  Get the latest from TODAY Sign up for our news...      1      fake   \n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...      1      fake   \n",
       "2  It’s safe to say that Instagram Stories has fa...      0  reliable   \n",
       "3  Much like a certain Amazon goddess with a lass...      0  reliable   \n",
       "4  At a time when the perfect outfit is just one ...      0  reliable   \n",
       "\n",
       "                                     no_punc_content  \\\n",
       "0  [Get, the, latest, from, TODAY, Sign, up, for,...   \n",
       "1  [Conan, On, The, Funeral, Trump, Will, Be, Inv...   \n",
       "2  [safe, to, say, that, Instagram, Stories, has,...   \n",
       "3  [Much, like, a, certain, Amazon, goddess, with...   \n",
       "4  [At, a, time, when, the, perfect, outfit, is, ...   \n",
       "\n",
       "                                       stemmed_words  \n",
       "0  [get, the, latest, from, today, sign, up, for,...  \n",
       "1  [conan, On, the, funer, trump, will, Be, invit...  \n",
       "2  [safe, to, say, that, instagram, stori, ha, fa...  \n",
       "3  [much, like, a, certain, amazon, goddess, with...  \n",
       "4  [At, a, time, when, the, perfect, outfit, is, ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stemmed_content(row):\n",
    "    text = row['no_punc_content']\n",
    "    stemmed_list = [ps.stem(word) for word in text]\n",
    "    return (stemmed_list)\n",
    "\n",
    "train['stemmed_words'] = train.apply(stemmed_content, axis=1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>no_punc_content</th>\n",
       "      <th>stemmed_words</th>\n",
       "      <th>no_stop_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>[Get, the, latest, from, TODAY, Sign, up, for,...</td>\n",
       "      <td>[get, the, latest, from, today, sign, up, for,...</td>\n",
       "      <td>[get, latest, today, sign, newslett, No, one, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>[Conan, On, The, Funeral, Trump, Will, Be, Inv...</td>\n",
       "      <td>[conan, On, the, funer, trump, will, Be, invit...</td>\n",
       "      <td>[conan, On, funer, trump, Be, invit, To, conan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[safe, to, say, that, Instagram, Stories, has,...</td>\n",
       "      <td>[safe, to, say, that, instagram, stori, ha, fa...</td>\n",
       "      <td>[safe, say, instagram, stori, ha, far, surpass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[Much, like, a, certain, Amazon, goddess, with...</td>\n",
       "      <td>[much, like, a, certain, amazon, goddess, with...</td>\n",
       "      <td>[much, like, certain, amazon, goddess, height,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[At, a, time, when, the, perfect, outfit, is, ...</td>\n",
       "      <td>[At, a, time, when, the, perfect, outfit, is, ...</td>\n",
       "      <td>[At, time, perfect, outfit, one, click, high, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label      type  \\\n",
       "0  Get the latest from TODAY Sign up for our news...      1      fake   \n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...      1      fake   \n",
       "2  It’s safe to say that Instagram Stories has fa...      0  reliable   \n",
       "3  Much like a certain Amazon goddess with a lass...      0  reliable   \n",
       "4  At a time when the perfect outfit is just one ...      0  reliable   \n",
       "\n",
       "                                     no_punc_content  \\\n",
       "0  [Get, the, latest, from, TODAY, Sign, up, for,...   \n",
       "1  [Conan, On, The, Funeral, Trump, Will, Be, Inv...   \n",
       "2  [safe, to, say, that, Instagram, Stories, has,...   \n",
       "3  [Much, like, a, certain, Amazon, goddess, with...   \n",
       "4  [At, a, time, when, the, perfect, outfit, is, ...   \n",
       "\n",
       "                                       stemmed_words  \\\n",
       "0  [get, the, latest, from, today, sign, up, for,...   \n",
       "1  [conan, On, the, funer, trump, will, Be, invit...   \n",
       "2  [safe, to, say, that, instagram, stori, ha, fa...   \n",
       "3  [much, like, a, certain, amazon, goddess, with...   \n",
       "4  [At, a, time, when, the, perfect, outfit, is, ...   \n",
       "\n",
       "                                     no_stop_stemmed  \n",
       "0  [get, latest, today, sign, newslett, No, one, ...  \n",
       "1  [conan, On, funer, trump, Be, invit, To, conan...  \n",
       "2  [safe, say, instagram, stori, ha, far, surpass...  \n",
       "3  [much, like, certain, amazon, goddess, height,...  \n",
       "4  [At, time, perfect, outfit, one, click, high, ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stop(row):\n",
    "    tokens = row['stemmed_words']\n",
    "    newtokens = [w for w in tokens if w not in english_stopwords]\n",
    "    return newtokens\n",
    "\n",
    "train['no_stop_stemmed'] = train.apply(remove_stop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>no_punc_content</th>\n",
       "      <th>stemmed_words</th>\n",
       "      <th>no_stop_stemmed</th>\n",
       "      <th>pre_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>[Get, the, latest, from, TODAY, Sign, up, for,...</td>\n",
       "      <td>[get, the, latest, from, today, sign, up, for,...</td>\n",
       "      <td>[get, latest, today, sign, newslett, No, one, ...</td>\n",
       "      <td>get latest today sign newslett No one ever tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>[Conan, On, The, Funeral, Trump, Will, Be, Inv...</td>\n",
       "      <td>[conan, On, the, funer, trump, will, Be, invit...</td>\n",
       "      <td>[conan, On, funer, trump, Be, invit, To, conan...</td>\n",
       "      <td>conan On funer trump Be invit To conan tb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[safe, to, say, that, Instagram, Stories, has,...</td>\n",
       "      <td>[safe, to, say, that, instagram, stori, ha, fa...</td>\n",
       "      <td>[safe, say, instagram, stori, ha, far, surpass...</td>\n",
       "      <td>safe say instagram stori ha far surpass compet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[Much, like, a, certain, Amazon, goddess, with...</td>\n",
       "      <td>[much, like, a, certain, amazon, goddess, with...</td>\n",
       "      <td>[much, like, certain, amazon, goddess, height,...</td>\n",
       "      <td>much like certain amazon goddess height direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>reliable</td>\n",
       "      <td>[At, a, time, when, the, perfect, outfit, is, ...</td>\n",
       "      <td>[At, a, time, when, the, perfect, outfit, is, ...</td>\n",
       "      <td>[At, time, perfect, outfit, one, click, high, ...</td>\n",
       "      <td>At time perfect outfit one click high demand t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label      type  \\\n",
       "0  Get the latest from TODAY Sign up for our news...      1      fake   \n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...      1      fake   \n",
       "2  It’s safe to say that Instagram Stories has fa...      0  reliable   \n",
       "3  Much like a certain Amazon goddess with a lass...      0  reliable   \n",
       "4  At a time when the perfect outfit is just one ...      0  reliable   \n",
       "\n",
       "                                     no_punc_content  \\\n",
       "0  [Get, the, latest, from, TODAY, Sign, up, for,...   \n",
       "1  [Conan, On, The, Funeral, Trump, Will, Be, Inv...   \n",
       "2  [safe, to, say, that, Instagram, Stories, has,...   \n",
       "3  [Much, like, a, certain, Amazon, goddess, with...   \n",
       "4  [At, a, time, when, the, perfect, outfit, is, ...   \n",
       "\n",
       "                                       stemmed_words  \\\n",
       "0  [get, the, latest, from, today, sign, up, for,...   \n",
       "1  [conan, On, the, funer, trump, will, Be, invit...   \n",
       "2  [safe, to, say, that, instagram, stori, ha, fa...   \n",
       "3  [much, like, a, certain, amazon, goddess, with...   \n",
       "4  [At, a, time, when, the, perfect, outfit, is, ...   \n",
       "\n",
       "                                     no_stop_stemmed  \\\n",
       "0  [get, latest, today, sign, newslett, No, one, ...   \n",
       "1  [conan, On, funer, trump, Be, invit, To, conan...   \n",
       "2  [safe, say, instagram, stori, ha, far, surpass...   \n",
       "3  [much, like, certain, amazon, goddess, height,...   \n",
       "4  [At, time, perfect, outfit, one, click, high, ...   \n",
       "\n",
       "                                  pre_processed_text  \n",
       "0  get latest today sign newslett No one ever tru...  \n",
       "1          conan On funer trump Be invit To conan tb  \n",
       "2  safe say instagram stori ha far surpass compet...  \n",
       "3  much like certain amazon goddess height direct...  \n",
       "4  At time perfect outfit one click high demand t...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rejoin(row):\n",
    "    my_list = row['no_stop_stemmed']\n",
    "    joined_text = ( \" \".join(my_list))\n",
    "    return joined_text\n",
    "\n",
    "train['pre_processed_text'] = train.apply(rejoin, axis=1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train['pre_processed_text'], y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aaaaand', 'aabb', 'aad', 'aaliyah', 'aandahl', 'aaron', 'aarp', 'aasen']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7053462940461726\n",
      "[[953  30]\n",
      " [455 208]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.97      0.80       983\n",
      "           1       0.87      0.31      0.46       663\n",
      "\n",
      "    accuracy                           0.71      1646\n",
      "   macro avg       0.78      0.64      0.63      1646\n",
      "weighted avg       0.76      0.71      0.66      1646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "\n",
    "#confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.7132442284325637\n",
      "Alpha:  0.1\n",
      "Score:  0.7624544349939246\n",
      "Alpha:  0.2\n",
      "Score:  0.7551640340218712\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.7430133657351154\n",
      "Alpha:  0.4\n",
      "Score:  0.735722964763062\n",
      "Alpha:  0.5\n",
      "Score:  0.7339003645200486\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.7296476306196841\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.7253948967193196\n",
      "Alpha:  0.8\n",
      "Score:  0.7174969623329283\n",
      "Alpha:  0.9\n",
      "Score:  0.7120291616038882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    y_pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 features\n",
    "print(len(tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features = 500, stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7278250303766707\n",
      "[[894  89]\n",
      " [359 304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80       983\n",
      "           1       0.77      0.46      0.58       663\n",
      "\n",
      "    accuracy                           0.73      1646\n",
      "   macro avg       0.74      0.68      0.69      1646\n",
      "weighted avg       0.74      0.73      0.71      1646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "\n",
    "#confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [MultinomialNB(),LinearSVC(), XGBClassifier(n_estimator=200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_process(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        def proc(z):\n",
    "            text = z.split()\n",
    "            text =  [w for w in text if w.isalpha()]\n",
    "            text = [w for w in text if w not in punctuation]\n",
    "            text = [ps.stem(word) for word in text]\n",
    "            text = [w for w in text if w not in english_stopwords]\n",
    "            text = \" \".join(text)\n",
    "            return text\n",
    "        z = X.apply(proc)        \n",
    "        return z\n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train['pre_processed_text'], y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Name of classifier being used with count vectorizer: \n",
      "\n",
      "MultinomialNB()\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76       983\n",
      "           1       0.65      0.59      0.62       663\n",
      "\n",
      "    accuracy                           0.71      1646\n",
      "   macro avg       0.69      0.69      0.69      1646\n",
      "weighted avg       0.70      0.71      0.70      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.7053462940461726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Name of classifier being used with count vectorizer: \n",
      "\n",
      "LinearSVC()\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75       983\n",
      "           1       0.64      0.54      0.59       663\n",
      "\n",
      "    accuracy                           0.69      1646\n",
      "   macro avg       0.68      0.67      0.67      1646\n",
      "weighted avg       0.69      0.69      0.69      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.6913730255164034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:59:39] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:59:39] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "\n",
      "Name of classifier being used with count vectorizer: \n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimator=200, n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78       983\n",
      "           1       0.69      0.58      0.63       663\n",
      "\n",
      "    accuracy                           0.73      1646\n",
      "   macro avg       0.72      0.70      0.71      1646\n",
      "weighted avg       0.72      0.73      0.72      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.7260024301336574\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    tfidf_pineline = Pipeline([\n",
    "        #('preproc', pre_process()),\n",
    "              ('cv', CountVectorizer(stop_words='english', max_features=500)),\n",
    "              ('clf',  classifier  )])\n",
    "    tfidf_pineline.fit(X_train,y_train)\n",
    "    y_pred = tfidf_pineline.predict(X_test)\n",
    "    print (\"\\n\\nName of classifier being used with count vectorizer: \\n\\n\" + str(classifier) + \n",
    "           \"\\n\\nClassification Report : \\n\\n\"+ str(metrics.classification_report(y_test,y_pred))  + \"\\n\\n Accuracy of classifier : \" + str(tfidf_pineline.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Name of classifier being used with tfidf vectorizer: \n",
      "\n",
      "MultinomialNB()\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.81       983\n",
      "           1       0.77      0.50      0.61       663\n",
      "\n",
      "    accuracy                           0.74      1646\n",
      "   macro avg       0.75      0.70      0.71      1646\n",
      "weighted avg       0.75      0.74      0.73      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.741190765492102\n",
      "\n",
      "\n",
      "Name of classifier being used with tfidf vectorizer: \n",
      "\n",
      "LinearSVC()\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77       983\n",
      "           1       0.67      0.60      0.63       663\n",
      "\n",
      "    accuracy                           0.72      1646\n",
      "   macro avg       0.71      0.70      0.70      1646\n",
      "weighted avg       0.72      0.72      0.72      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.7199270959902795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:00:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:00:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "\n",
      "Name of classifier being used with tfidf vectorizer: \n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimator=200, n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       983\n",
      "           1       0.75      0.52      0.61       663\n",
      "\n",
      "    accuracy                           0.74      1646\n",
      "   macro avg       0.74      0.70      0.71      1646\n",
      "weighted avg       0.74      0.74      0.72      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.7363304981773997\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    tfidf_pineline = Pipeline([\n",
    "        #('preproc', pre_process()),\n",
    "              ('cv', TfidfVectorizer(max_features=1000,stop_words='english',ngram_range=(2,2))),\n",
    "              ('clf',  classifier  )])\n",
    "    tfidf_pineline.fit(X_train,y_train)\n",
    "    y_pred = tfidf_pineline.predict(X_test)\n",
    "    print (\"\\n\\nName of classifier being used with tfidf vectorizer: \\n\\n\" + str(classifier) + \n",
    "           \"\\n\\nClassification Report : \\n\\n\"+ str(metrics.classification_report(y_test,y_pred))  + \"\\n\\n Accuracy of classifier : \" + str(tfidf_pineline.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Name of classifier being used with tfidf vectorizer: \n",
      "\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       983\n",
      "           1       0.75      0.53      0.62       663\n",
      "\n",
      "    accuracy                           0.74      1646\n",
      "   macro avg       0.74      0.70      0.71      1646\n",
      "weighted avg       0.74      0.74      0.73      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.7387606318347509\n"
     ]
    }
   ],
   "source": [
    "tfidf_pineline = Pipeline([\n",
    "#('preproc', pre_process()),\n",
    "('cv', TfidfVectorizer(max_features=1000,stop_words='english',ngram_range=(2,2))),\n",
    "('clf',  LogisticRegression())])\n",
    "tfidf_pineline.fit(X_train,y_train)\n",
    "y_pred = tfidf_pineline.predict(X_test)\n",
    "print (\"\\n\\nName of classifier being used with tfidf vectorizer: \\n\\n\" + str(LogisticRegression) + \n",
    "           \"\\n\\nClassification Report : \\n\\n\"+ str(metrics.classification_report(y_test,y_pred))  + \"\\n\\n Accuracy of classifier : \" + str(tfidf_pineline.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Name of classifier being used with tfidf vectorizer: \n",
      "\n",
      "<class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       983\n",
      "           1       0.63      0.63      0.63       663\n",
      "\n",
      "    accuracy                           0.70      1646\n",
      "   macro avg       0.69      0.69      0.69      1646\n",
      "weighted avg       0.70      0.70      0.70      1646\n",
      "\n",
      "\n",
      " Accuracy of classifier : 0.6986634264884569\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_pineline = Pipeline([\n",
    "#('preproc', pre_process()),\n",
    "('cv', TfidfVectorizer(max_features=1000,stop_words='english',ngram_range=(2,2))),\n",
    "('clf',  DecisionTreeClassifier())])\n",
    "\n",
    "tfidf_pineline.fit(X_train,y_train)\n",
    "y_pred = tfidf_pineline.predict(X_test)\n",
    "print (\"\\n\\nName of classifier being used with tfidf vectorizer: \\n\\n\" + str(DecisionTreeClassifier) + \n",
    "           \"\\n\\nClassification Report : \\n\\n\"+ str(metrics.classification_report(y_test,y_pred))  + \"\\n\\n Accuracy of classifier : \" + str(tfidf_pineline.score(X_test,y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
